{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from docopt import docopt\n",
    "from scipy import sparse\n",
    "import os\n",
    "import subprocess\n",
    "from shutil import copyfile\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nworkflow of the verify_implementation script\\n\\nverify glove bias\\n1. I build W and H matrix, build bias term. And then, multply W and H, add bias term, use this matrix as my question\\n2. store the question matrix nnz term and meta information to ./matrix_folder\\n3. build a weight matrix and store its question and nnz term to ./count_folder\\n4. modified code, let it output bias term when factorize the matrix\\n5. read in the answer matrix, test them, to see whether it can output the bias term\\n\\nverify glove weight\\n1. I build W and H matrix, multply W and H, use this matrix as my question\\n2. build a weight matrix, assign one position very high weight, store nnz and meta file in count_folder\\n3. open glove weight, close glove bias, factorize the matrix\\n4. 5. read in the answer matrix, test them, to see the error on the position with high weight\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "workflow of the verify_implementation script\n",
    "\n",
    "verify glove bias\n",
    "1. I build W and H matrix, build bias term. And then, multply W and H, add bias term, use this matrix as my question\n",
    "2. store the question matrix nnz term and meta information to ./matrix_folder\n",
    "3. (this step is not important, because weight is closed when testing bias term)\n",
    "   build a weight matrix and store its question and nnz term to ./count_folder\n",
    "4. modified code, let it output the bias term when factorize the matrix\n",
    "5. read in the answer matrix, test them, to see whether it can output the bias term\n",
    "\n",
    "verify glove weight\n",
    "1. I build W and H matrix, multply W and H, use this matrix as my question matrix\n",
    "2. build a weight matrix, assign one position very high weight, store nnz and meta file in count_folder\n",
    "3. open glove weight, close glove bias, factorize the matrix\n",
    "4. read in the answer matrix, test them, to see the error on the position with high weight\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "input: matrix_size = the size of question matrix which I need to factorize (how many words in total)\n",
    "input: embedding_rank\n",
    "input: include_bias (default = 0), set it as 1, when you need to add bias term when generating question matrix.\n",
    "       only when include_bias = 1, we need to care the W_bias and H_bias in the return object.\n",
    "       when include_bias = 0, the W_bias and H_bias in the return object are useless.\n",
    "       \n",
    "output: W, H, W_bias, H_bias, question_matrix; all of them are np array\n",
    "        question_matrix[i][j] = W[i] * H[j] + W_bias[i] + H_bias[j]\n",
    "        W: matrix_size * embedding_rank, np array\n",
    "        H: embedding_rank * matrix_size, np array\n",
    "        W_bias: matrix_size, np array\n",
    "        H_bias: matrix_size, np array\n",
    "        question_matrix: matrix_size * matrix_size, np array\n",
    "'''\n",
    "def build_answer_and_question(matrix_size, embedding_rank, include_bias = 0):\n",
    "    W = np.random.random((matrix_size, embedding_rank))\n",
    "    H = np.random.random((embedding_rank, matrix_size))\n",
    "\n",
    "#     W = W*(W>0.55)\n",
    "#     H = H*(H<0.45)\n",
    "    \n",
    "    W_bias = np.random.random(matrix_size)\n",
    "    H_bias = np.random.random(matrix_size)\n",
    "    \n",
    "    question_matrix = np.dot(W, H)\n",
    "    \n",
    "    if include_bias:\n",
    "\n",
    "        print \"inlcude_bias is on\"\n",
    "        question_matrix = np.transpose(question_matrix)\n",
    "        for i in range(question_matrix.shape[0]):\n",
    "            question_matrix[i] = np.add(question_matrix[i], W_bias)\n",
    "        question_matrix = np.transpose(question_matrix)\n",
    "        \n",
    "        for i in range(question_matrix.shape[1]):\n",
    "            question_matrix[i] = np.add(question_matrix[i], H_bias)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print question_matrix[1][5]\n",
    "    print (np.dot(W[1], H[:,5]) + W_bias[1] + H_bias[5])\n",
    "    return W, H ,W_bias, H_bias, question_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "input: matrix_size, I need to build a matrix_size * matrix_size weight matrix, everywhere except [1][1] is 1\n",
    "input: weight_for_exception_term, how large weight you want to assign to the exception position\n",
    "\n",
    "output: a matrix_size * matrix_size weight matrix, np array, the number in the [1][1] is weight_for_exception_term\n",
    "'''\n",
    "def build_weight_matrix(matrix_size, weight_for_exception_term):\n",
    "    weight_matrix = np.ones((matrix_size, matrix_size))\n",
    "    weight_matrix[1][1] = 1\n",
    "    \n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "input is a np array, path_and_name\n",
    "\n",
    "this function will save the nnz in path_and_name in the following foramt:\n",
    "x_coordinate y_coordinate value\n",
    "please note, in the final document, x_coordinate and y_coordinate start from 1, not 0; program will add 1 to each term's x and y coordinate.\n",
    "\n",
    "'''\n",
    "def save_nonzero_term_fast(nparray, path_and_name):\n",
    "    csrmatrix = sparse.csr_matrix(nparray)  \n",
    "    with open(path_and_name,'w') as f:\n",
    "        for i in range(len(csrmatrix.indptr)-1):\n",
    "#             if i % 1000000 == 0:\n",
    "#                 print i, len(csrmatrix.indptr)-1\n",
    "            columnIndices=[]\n",
    "            dataInLine=[]\n",
    "            columnIndices=csrmatrix.indices[csrmatrix.indptr[i]:csrmatrix.indptr[i+1]] \n",
    "            dataInLine=csrmatrix.data[csrmatrix.indptr[i]:csrmatrix.indptr[i+1]]\n",
    "            for j in range(len(columnIndices)):\n",
    "                f.write(\"%d %d %.6f\\n\"% (i+1, columnIndices[j]+1, dataInLine[j]))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "input is a np array, path_and_name\n",
    "\n",
    "this function will save the information in path_and_name in the following foramt:\n",
    "matrix_size matrix_size\n",
    "nnz_number training.ratings\n",
    "nnz_number test.ratings\n",
    "for example:\n",
    "71290 71290\n",
    "1289567 training.ratings\n",
    "1289567 test.ratings\n",
    "'''\n",
    "def create_meta_file(nparray, path_and_name):\n",
    "    csrmatrix = sparse.csr_matrix(nparray)\n",
    "    nnz_number = csrmatrix.getnnz()\n",
    "    size = nparray.shape[0]\n",
    "\n",
    "    with open(path_and_name,'w') as f:\n",
    "        f.write(\"%d %d\\n\" %(size, size))\n",
    "        f.write(\"%d %s\\n\" %(nnz_number, 'training.ratings'))\n",
    "        f.write(\"%d %s\" %(nnz_number, 'test.ratings'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "this function will read in the left and right matrix in the path_and_name\n",
    "input: path_and_name\n",
    "this funciton will read in the path_and_name +'.W' and path_and_name +'.H' file\n",
    "\n",
    "output: left matrix a and right matrix b\n",
    "        left matrix a: matrix_size * embedding_rank, np array\n",
    "        right matrix b: embedding_rank * matrix_size, np array\n",
    "'''\n",
    "def read_answer(path_and_name):\n",
    "    a = np.loadtxt(path_and_name +'.W')\n",
    "#     print (\"a.shape\",a.shape)\n",
    "    b = np.transpose(np.loadtxt(path_and_name +'.H'))\n",
    "#     print (\"b.shape\",b.shape)\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14859811964\n",
      "2.26181536265\n",
      "[[ 2.63947283  2.0406623   1.19458877  2.15369068  1.19950848  1.30697975\n",
      "   1.89603186  1.31834889]\n",
      " [ 2.24686787  1.61518499  0.84810509  1.59010401  0.90798864  1.14859812\n",
      "   1.56842441  1.05412623]\n",
      " [ 2.26894776  1.36705681  0.69838035  1.91368497  0.63435601  1.10819921\n",
      "   1.37179661  0.77100578]\n",
      " [ 3.00803645  2.26961125  1.37414111  2.59521223  1.10857258  1.56039249\n",
      "   1.70545589  1.37932572]\n",
      " [ 2.48556907  1.89074542  1.1144728   1.70118599  0.98328166  1.29495298\n",
      "   1.54848795  1.18431796]\n",
      " [ 1.92636216  1.30757743  0.83123069  1.56457634  0.83455125  0.96362978\n",
      "   1.44189385  1.01705652]\n",
      " [ 2.8755597   1.87198297  1.30679197  2.07522847  1.02026398  1.42668388\n",
      "   1.8393194   1.30124779]\n",
      " [ 1.21705964  0.86208757  0.55048013  1.17303518  0.54837343  0.6493422\n",
      "   0.86623937  0.73323406]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "verify glove weight\n",
    "set include_bias as 0 when build qutstion matrix\n",
    "save nnz terms and meta information\n",
    "build weight matrix and save cooresponding information, assign a very high weight to [1][1] position\n",
    "factorize the matrix, set G(glove_bias) as 0, set W(glvove_weight) as 1. set x_max as a very high number\n",
    "read in and delete answer matrix\n",
    "see the difference in every position between original matrix and rebuilded matrix \n",
    "'''\n",
    "\n",
    "matrix_size = 8\n",
    "rank = 6\n",
    "\n",
    "W, H ,W_bias, H_bias, question_matrix = build_answer_and_question(matrix_size, rank, 0)\n",
    "create_meta_file(question_matrix, \"./matrix_folder/meta\")\n",
    "save_nonzero_term_fast(question_matrix, \"./matrix_folder/training.ratings\")\n",
    "save_nonzero_term_fast(question_matrix, \"./matrix_folder/test.ratings\")\n",
    "\n",
    "weight_matrix = build_weight_matrix(matrix_size, 1000)\n",
    "create_meta_file(weight_matrix, \"./count_folder/meta\")\n",
    "save_nonzero_term_fast(weight_matrix, \"./count_folder/training.ratings\")\n",
    "save_nonzero_term_fast(weight_matrix, \"./count_folder/test.ratings\")\n",
    "\n",
    "subprocess.check_output([\"make\"])\n",
    "subprocess.check_output([\"./converter\", \"./matrix_folder\"])\n",
    "subprocess.check_output([\"./omp-pmf-train\", \"-s\", \"10\", \"-n\", \"10\", \"-f\", \"1\", \"-t\", \"20\", \"-q\", \"1\", \"-p\", \"0\", \"-r\", \"0.015625\", \"-l\", \"0.015625\", \"-b\", \"0\", \"-k\", str(rank), \"-E\", \"0\", \"-X\", \"1000000\", \"-W\", \"1\", \"-G\", \"0\", \"matrix_folder\", \"count_folder\", \"test_code\"])\n",
    "\n",
    "W_answer, H_answer = read_answer('test_code-l0.015625-r0.015625-iter20-gweight1-xmax1000000-gbias0.final')\n",
    "subprocess.check_output([\"rm\", 'test_code-l0.015625-r0.015625-iter20-gweight1-xmax1000000-gbias0.final.W'])\n",
    "subprocess.check_output([\"rm\", 'test_code-l0.015625-r0.015625-iter20-gweight1-xmax1000000-gbias0.final.H'])\n",
    "\n",
    "print np.subtract(question_matrix, np.dot(W_answer, H_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inlcude_bias is on\n",
      "2.11814568245\n",
      "2.11814568245\n",
      "[ 0.742  0.035  0.565  0.788  0.807  0.797  0.974  0.709]\n",
      "[ 0.025 -0.085  0.064  0.018  0.001  0.001  0.083  0.05 ]\n",
      "[ 0.884  0.645  0.792  0.581  0.905  0.975  0.075  0.2  ]\n",
      "[ 0.17   0.148  0.203  0.146  0.222  0.197  0.06   0.105]\n",
      "[(1, 0.034687117733612549), (2, 0.5646343551039168), (7, 0.70892671481778602), (0, 0.74154172135293617), (3, 0.78792273763768861), (5, 0.79665764254060456), (4, 0.80666487565685507), (6, 0.9739816139185018)]\n",
      "[(1, -0.084616999999999998), (5, 0.001132), (4, 0.0014790000000000001), (3, 0.017877000000000001), (0, 0.025127), (7, 0.049993000000000003), (2, 0.064140000000000003), (6, 0.082632999999999998)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "verify glove bias\n",
    "set include_bias as 1 when build qutstion matrix\n",
    "save nnz terms and meta information\n",
    "build weight matrix and save cooresponding information(this step is useless)\n",
    "factorize the matrix, set G(glove_bias) as 1, set W(glvove_weight) as 0.\n",
    "read in and delete answer matrix\n",
    "see the difference in every position between original matrix and rebuilded matrix \n",
    "see the difference between original bias and the bias in my answer\n",
    "'''\n",
    "# def main():\n",
    "#     args = docopt(\"\"\"\n",
    "#     Usage: \n",
    "#         verify_inner_product.py <representation_shared_path>\n",
    "#     \"\"\")\n",
    "\n",
    "matrix_size = 8\n",
    "rank = 5\n",
    "W, H ,W_bias, H_bias, question_matrix = build_answer_and_question(matrix_size, rank, 1)\n",
    "\n",
    "# print question_matrix\n",
    "\n",
    "create_meta_file(question_matrix, \"./matrix_folder/meta\")\n",
    "save_nonzero_term_fast(question_matrix, \"./matrix_folder/training.ratings\")\n",
    "save_nonzero_term_fast(question_matrix, \"./matrix_folder/test.ratings\")\n",
    "\n",
    "create_meta_file(question_matrix, \"./count_folder/meta\")\n",
    "save_nonzero_term_fast(question_matrix, \"./count_folder/training.ratings\")\n",
    "save_nonzero_term_fast(question_matrix, \"./matrix_folder/test.ratings\")\n",
    "\n",
    "subprocess.check_output([\"make\"])\n",
    "subprocess.check_output([\"./converter\", \"./matrix_folder\"])\n",
    "subprocess.check_output([\"./omp-pmf-train\", \"-s\", \"10\", \"-n\", \"10\", \"-f\", \"1\", \"-t\", \"10\", \"-q\", \"1\", \"-p\", \"0\", \"-r\", \"0.015625\", \"-l\", \"0.03125\", \"-b\", \"0\", \"-k\", str(rank), \"-E\", \"0\", \"-X\", \"1\", \"-W\", \"0\", \"-G\", \"2\", \"matrix_folder\", \"count_folder\", \"test_code\"])\n",
    "\n",
    "W_answer, H_answer = read_answer('test_code-l0.031250-r0.015625-iter10-gweight0-xmax1-gbias2.final')\n",
    "subprocess.check_output([\"rm\", 'test_code-l0.031250-r0.015625-iter10-gweight0-xmax1-gbias2.final.W'])\n",
    "subprocess.check_output([\"rm\", 'test_code-l0.031250-r0.015625-iter10-gweight0-xmax1-gbias2.final.H'])\n",
    "\n",
    "print W_bias\n",
    "print W_answer[:,rank+1]\n",
    "\n",
    "print H_bias\n",
    "print H_answer[rank,:]\n",
    "\n",
    "W_bias_dict = dict([(i, a) for i, a in enumerate(W_bias)])\n",
    "sort_W_bias_dict = sorted(W_bias_dict.items(), key=itemgetter(1))\n",
    "print sort_W_bias_dict\n",
    "\n",
    "W_answer_dict = dict([(i, a) for i, a in enumerate(W_answer[:,rank+1])])\n",
    "sort_W_answer_dict = sorted(W_answer_dict.items(), key=itemgetter(1))\n",
    "print sort_W_answer_dict\n",
    "\n",
    "H_bias_dict = dict([(i, a) for i, a in enumerate(H_bias)])\n",
    "sort_H_bias_dict = sorted(H_bias_dict.items(), key=itemgetter(1))\n",
    "# print H_bias_dict\n",
    "\n",
    "H_answer_dict = dict([(i, a) for i, a in enumerate(H_answer[rank,:])])\n",
    "sort_H_answer_dict = sorted(H_answer_dict.items(), key=itemgetter(1))\n",
    "# print H_answer_dict\n",
    "\n",
    "# print np.subtract(W_bias, W_answer[:,rank+1])\n",
    "# print np.subtract(H_bias, H_answer[rank,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
